# ICLR 2020
- ICLR 2020, is being held for the first time virtually, I volunteered for the conference in testing the services.
- This is my first conference and I'm pretty excited about it.

# Climate Change with ML Forecasts (Panel)
- Think about Flood forecasting
    - Climacell, Boston 
    - East Africa has seen unprecedented changes in climate, and the disaster of the flooding would've been handled if there were local information and more data points across the place.
- Cobuilding solutions with academic communities, users, local government bodies, working with stakeholders, experts on the problem.
- Explainable AI 
    - Causing problems with deployments because there is no trust. 
    - Emerging market bodies
        - Trust breaks down when not working with local govt bodies and takes time to seek out the right people
- https://www.climatechange.ai
    - Tackling climate change with machine learning https://arxiv.org/pdf/1906.05433.pdf

# Safe and Reliable Machine
- https://iclr.cc/virtual/workshops_6.html



- Compressive Transformer
- StructBert
- Understanding Knowledge Distillation in Non-autoregressive Machine Translation 
    - 



# Poly-Encoders: Architectures and Pretraining Strategies for Fast and Accurate Multi Sentence Scoring - https://openreview.net/pdf?id=SkxgnnNFvH
- Rank Candidate Responses Quickly and Accurately
- Model
    - Transformer Encoder 
    - BERT -> MLM + NSP
- Architecture
    - Bi-Encoder -> Two BERTs, Input and each candidate, get scores by dot product between them
    - Cross-encoder -> Concatenate and then add send through BERT
- Cross encoder has better performance, Bi-encoder is faster
- Poly-Encoder
    - 


# Cross-lingual Alignment vs Joint Training: A Comparative Study and A Simple Unified Framework - https://openreview.net/pdf?id=S1l-C0NtwS

- Conduct Systematic studies of two popular cross-lingual word embeddings -> Cross Lingual Alignment vs Joint Training
- Contributes a unified framework
- Cross lingual embeddings mean words with similar meanings should be close to each other, to facilitate cross lingual knowledge transfer
    - Goal is to learn a shared vector space for multiple languages.
- Cross Lingual 
    - Train embeddings differently, obtain dictionary and learn a projection matrix 
    - Problems of under sharing and no words are aligned
- Joint training
    - All embeddings trained at the same time, obtains joint vocabulary and trains using concatenated mono-lingual corpora
    - Problem of over sharing and language specific word are poorly aligned
- Unified Framework
    - set of shared embeddings obtained using unsupervised learning
    - Vocabulary reallocation 
        - Words appearing exclusively in a language are reallocated and words appearing with similar frequency are kept at the same level.
    - Alignment Refinement 
        - Off the shelf alignment methods used to refine alignments across the non sharing embedding sets.
- Relative performance is task specific. 
- 

# Generalization through Memorization: Nearest Neighbor Language Models - https://openreview.net/pdf?id=HklBjCEKvH
- Asks the question, "Can explicitly memoryizing the training data helps generalization without the added cost of training"
- Can scale to larger text corpora and helps domain adaptation
- Start with any Neural autoregressive LM, feed it our test contest, this gives a distribution over the next word and a fixed sized embedding for the context
    - This fixed sized representation is used to query the nearest neighbour datastore offline
- Constructing the datastore
    - Forward pass over the examples with a PLM
    - This gives context representations over the examples and this gives the keys of the datastore and the targets are the values, to be used during inference.
- Query is a test context representations and the differences are calculated between it and a subset of keys from the datastore
    - Prune and keep the top k
    - normalize the distances and aggregate over multiple occurences to give knn distribution
    - Last interpolate LM and knn components to get final KNN distribution
- Wikitext 103 best perplexity










Papers Missed -> https://openreview.net/pdf?id=HJe_Z04Yvr - Adjustable Real Time transfer


Would like to mention that evaluating tasks like document summarization (abstractive) is a very difficult task because its very hard to build a formal notion of what abstraction is, and that current datasets (like CNN/Daily Mail and Newsroom) dont really capture all aspects of the problem. While an objective evaluation is pretty difficult, there have been a variety of papers with a way to interpret the model outputs. 
A variety of biases in datasets and metrics were mentioned in this paper https://www.aclweb.org/anthology/D19-1327.pdf.

# A Probabilistic Formulation of Unsupervised Text Transfer - https://openreview.net/pdf?id=HJlA0C4tPS, https://github.com/cindyxinyiwang/deep-latent-sequence-model
- Text Style Transfer -> Change the style of the sentence and keep the semantic meaning unchanged.
- sentiment transfer, formality transfer, author imitation, machine translation -> other kind of text style transfer
- Adversarial Loss + Auto Encoding loss (shen et al 2017, yang et al 2018)
- Lample et al 2019
- The model uses amortized variational inference
    - Subtly different unsupervised objective
    - SOTA and has probabilistic perspectives
- Transduction assumption and LM model is trained over prior.
    

# Plug and Play Language models: A simple approach to controlled text generation - https://openreview.net/pdf?id=H1edEyBKDS
- Gpt2 generates generic sentences, the model can be steered to make sentiment based sentences
- PPLM 
    - a LM predicts the token distribution for the next token given the input context
    - Train a small attribute model which models positivity 
    - Control degeneration by modeling p(x)
- Language Detoxification
- Combining attribute models


# ELECTRA - https://openreview.net/pdf?id=HJlA0C4tPS
- ULMFit, ELMO, GPT -> generate sequence one token at a time
- BERT, XLNET, RoBERTa, ALBERT -> MLM pretraining, only learning from typically 15% of tokens which are masked out
- Replaced Token Detection 
    - Replace tokens which sort of fit into the context but dont exactly fit into the context
    - Bidirectional learning from all positions
    - The replacements need to be generated by a generator trained jointly during training
- 

# VL-BERT - https://iclr.cc/virtual/poster_SygXPaEYvH.html
- Visual-Linguistic Tasks
- The representations were earlier combined in a task specific way which is hard and time consuming
- VL BERT
    - Task agnostic architecture
    - Visual-Linguistic pretraining
        - in addition to bert
        - Add image regions
        - Visual Feature embedding
    - Visual Linguistic Corpus: Conceptual Captions
    - Text-only Corpus 
- Pretraining tasks
    - MLM with visual clues (the word is masked in the sentence, and the image is given to predict the word)
    - Masked ROI classfication with linguistic clues (a portion of the image is masked and the sentence is given to predict the masked area)
- Works very well on VCR, VQA and COCO.
- Pretraining procedure helps for better embeddings.

# On the weakness of Reinforcement Learning for NMT - https://openreview.net/pdf?id=H1eCw3EKvH
- In RL, training and inference based on system outputs. (no exposure bias)
    - In MT, First warmup with MLE and then use REINFORCE or MRT
    - MRT is not guaranteed to converge
- Reinforcement with constant 1 is as good as with BLUE
- Peakiness Effects
    - RL only learns from what it samples and with enough samples it'll maximize the reward
    - If the probability distribution peaky, many sample are needed
        - But if its already peaked or if a model sees many "okay" rewards, it'll become more probable
    - A prediction which gets high reward, its made more probable.
- MT is already peaked, and RL makes predictions even more peaked.
- The most probable tokens are extremely probable
    - The top predictions hardly ever change. 
- 1] How much do you think your work translates to other applications of RL in NLP wherein sequence generation is involved? Is peaky-ness an issue specific to NMT?
- 2] So you mention in the talk, that the problem has an inclination to output more probable words, what do you think would happen if there was a regularization objective which forced the model to consider other words as well. For eg: Unlikelihood training. { THIS MIGHT WORK - EXPERIMENT}


# Non Autoregressive for Dialogue State tracking - https://openreview.net/pdf?id=H1e_cC4twS
- Limitations 
    - Unable to detect unseen slot values
    - Tokens are generated one by one, expensive
    - Dependencies among slots are not explicitly learned
- NADST
    - Fertility decoder
        - Model dependencies among (domain, slot) pairs
    - State decoder
        - Construct input sequence as (domain, slot) x fertility
    - Enables fast decoding of dialogue states
-  

# From Inference to Generation: End-to-end Fully Self-supervised Generation of Human Face from Speech - https://openreview.net/pdf?id=H1guaREYPr
- Inference and generation of faces from speech
- Unified cross-modal inference and generation framework that can be trained in self-supervised way
- 
- Training inference module using negative sampling
- Train conditional GANs using trained inference networks
    - Relativisitic Conditional loss introduced


# ReCLOR : Reading Comprehension Dataset requiring logical reasoning - https://openreview.net/pdf?id=HJgJtT4tvB
- Logical reasoning is an ability to examine, analyze and critically evaluate.
- Logical Reasoning
    - 0% in MCTest and 1.2% in SQUAD
- ReCLOR
    - 6138 question 
- Different types of logical reasoning are incorporated


# Lite Transformer with Long-Short Range Attention - https://arxiv.org/pdf/2004.11886.pdf
- The FFN takes more than half of the computation
    - Not desirable
- Flattened transformer block, attention takes major computation 
- Attention in the original transformer and then added the following block.
    - GLU-> Conv -> FC
- Flattened the transformer block and LSRA introduced


# 2020 Vision - Ruha Benjamin
- Technology is in the driver seat Assumption
    - But we come up and imagine with this
- Power of ML 
    - Think about Horizontal Form of Power 
    - Personal Slaves "again", who is the beneficiary of the technology 
- Deep Learning
    - Think beyond computational depth and without historical and sociologicl depth is superficial
    - Do machine learning practitioner learn about technologies destructive usages of automation
        - What all other things are these things being used for 
    - Structural Inequity
        - The bench example, discriminatory design, hostile design 
            - Collectively respond to it 
        - Can be built into technology as well
            - What spikes are we building into physical and digital infrastructures
- Takeaways
    - Racism is productive, it constructs
    - Race and Technology are coproduced
    - imagination is a field of action
- Citizen App 
    - Race after Technology, Captivating technology books by ruha benjamin
    - How are race affecting coming up with technology
    - The californian app companies delivering apps to kenya (tala, branch companies) resulting into perpetual debt circuits
- Duplicity of technological fixes (The sleepdealers clip)
- The new jim
    - Engineered inequity
    - Default discrimination
        - Pro publica report on algorithm bias in parolees in a county
    - Coded exposure
        - Being Included or excluded in the wrong way 
    - Techno-Benevolence
- Advancing racial literacy in tech handbook
- Ruined by Design By mike monteiro


# IBM: NeuroSymbolic Hybrid AI 
- AI -> Machine Learning, DL, CV, NLP
    - General AI -> Revolutionary
    - Broad AI -> Disruptive and pervasive
    - Narrow AI -> Emerging
- Broad AI, Multitask, MultiDomain, Distributed AI, Explainable (This is what this lab deals with) 
- Gatys et al 2015, Brock et al 2018, Karpathy et al 2015 
- Objects out of context make the model fail, Wang et al 2018
- ObjectNet  
- Pin yu chen et al 2017 - Adversarial, Xu et al 2019
- CLEVR Dataset
- Symbolic AI 
- Neural-Symbolic AI (reading list)
    - Concept and Reasoning are usually entangled and hard for neural networks
    - NS-VQA (Yi et al 2018)
    - Neurosymbolic concept learner, ICLR 2019
    - Neurosymbolic Metaconcept Learner Neurips 2019
    - Dynamic Scenes Conterfactual ICLR 2020
    - CLEVERER 
    - Neurosymbolic generative models srivastava et al 2020
    - Neurosymbolic NLU Naacl 2019
    - Neurosymbolic Code optimization shi et al 2019


- 
READING 
# BERTScore - https://openreview.net/pdf?id=SkeHuCVFDr, https://arxiv.org/abs/1904.09675 (Check citations)
# Improving NLG with Spectrum Control - https://openreview.net/pdf?id=ByxY8CNtvr
# Unlikelihood Training
- Followup work - https://arxiv.org/pdf/1911.03860.pdf
- Sparse Text Generation - https://arxiv.org/pdf/2004.02644.pdf
- Summarization Evaluation
    -  Correlation with human judgements for different types of summarizations. We found the WMT human judgements data to be extremely useful for this work. I don't know that an equivalent resource exists for summarization. There's a bit for caption generation, but we are largely going in blind. Another issue is the diversity of correct outputs. It's very likely that there are so many correct outputs for summarization, many very different from others. This makes token matching evaluation a bit inappropriate. I am not a summarization expert by a long shot, so I might be wrong here. (Yoav artzi comments)


Tools list -> https://docs.google.com/spreadsheets/d/1jvJXusSCqqnDQD35RGEknVdZf4Ay7O3oZGafQgqPctA/edit#gid=0


# Playing the lottery with rewards and multiple languages: Lottery tickets in RL and NLP - https://openreview.net/pdf?id=S1xnXRVFwH
- Lottery ticket hypothesis suggests that current initialization has substantial room to improve.
- This paper shows the existence of this phenomena in RL and NLP
- Rewinding and Iterative pruning are both important for finding lottery tickets in NLP
- 

# Drawing Early Bird Tickets: Towards more efficient training of Deep Networks - https://openreview.net/pdf?id=BJxsrgStvr
- Progressive Pruning and Training (J.Frankle, ICLR 2019)
- Early Bird training proposed
    - Existence of early bird tickets discovered
    - propose a detector to find these early birds
        - Hamming distance based method
    - Efficient training via EB tackets
        - Reduced training cost by upto 80%

# Deep Learning for Symbolic Maths


# What can Neural Networks reason about? - https://openreview.net/pdf?id=rJxbJeHFPS
- Need AI which reason about the world
- Better "Algorithmic alignment" implies better generalization
    - Inductive bias of architectures formally defined
- GNN similar to bellman ford algorithm i.e they align with the structure of the algorithm
- GNN aligns with Dynamic programming as well.
- GNN has limitations specifically on The subset sum (NP-hard problem)
- Neural Exhaustive Search - Based on algorithm alignment achieves top performance on the task.
- If 

# A Mutual Information Maximization Perspective of Language Representation Learning - https://openreview.net/pdf?id=Syx79eBKwr
- Mutual Information Maximization 
- InfoNCE is Used (logeswaran et al 2018, van den oord et al 2019)
- InfoWord; Deep Infomax(DIM; Hjelm et al 2019)
- Performs better on squad wrt original metrics [CHECK]
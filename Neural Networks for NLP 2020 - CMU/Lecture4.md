# Lecture 4 -  Recurrent Neural networks
- We use RNNs for NLP because of the existence of sequential data, eg words in sentences, characters in words, sentences in discourse and to model long distance dependencies in language.
- (sidenote: check winograd and winogrande schema challenge.)
- Paper: RNN - Elman 1990 : Rnn's are "unrolled in time", i.e it goes over the sequence in each time step. it is the same function that is being applied. The loss is just the sum of all the predicted losses at each time step.  
-  

## Books
- Yoav Goldberg - Neural Network methods for Natural Language processing
## Papers & Blogs (In class and other interesting papers)
- AWD LSTM - Merity et al 2017
- An Overview of Gradient Descent Algos - Ruder Blog
- Dropout Srivastava et al 2014
- Stronger Baselines for NMT (Neubig et al 2017)
- Graph Convolution for text (2017)
- Empirical Evaluation of Activation Functions (Eger et al 2018)
- Optimizing RNNs with CuDNN (appleyard 2015)
- Learning Sentiment from LMs (Radford et al 2017)
- Visualizing LSTMs (Karpathy et al 2015.)
- Optimized Implementations of LSTMs (Appleyard 2015)
- Noise Contrastive Estimation (Mnih & Teh 2012)
- Von Mises-Fisher Loss for training Seq2Seq Models with Continous Outputs (Kumar and Tsetkov et al 2019) [[Paper](https://arxiv.org/pdf/1812.04616.pdf)][[Review](https://github.com/Shashi456/Papers/blob/master/Review/VonMisesLoss.md)]
- Ensemble Distillation(kim et al 2016)
- BERTScore (Zhang et al 2019)
- Conditioned LM 
    - From Structured Data (Wen et al 2015)
    - Challenges in data to document generation (Wiseman et al 2017)
- Attention Papers
    - Cohn et al 2015 
    - Attention is not Alignment (koegn and knowles 2017)/ Jain & Wallace 2019
    - Hierarchical Attention (Yang et al 2015)
    - Gu et al 2016/ Arthur et al 2016  
    - Bahdanau et al 2015
- Word Embeddings 
    - Levy and Goldberg et al 2014
    - Glove 2014
    - Word2Vec 2014
    - Intrinsic Evaluation of Embeddings (Schnabel et al 2015) 
    - Murphy et al 2012
    - Bag-of-character n-grams (wieting et al 2016)
    - De-biasing Word Embeddings (Bolukbasi et al 2016.)
    - 


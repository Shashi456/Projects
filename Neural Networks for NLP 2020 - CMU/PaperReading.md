## Books
- Yoav Goldberg - Neural Network methods for Natural Language processing
## Papers & Blogs (In class and other interesting papers)
- AWD LSTM - Merity et al 2017
- An Overview of Gradient Descent Algos - Ruder Blog
- Dropout Srivastava et al 2014
- Stronger Baselines for NMT (Neubig et al 2017)
- Graph Convolution for text (2017)
- Empirical Evaluation of Activation Functions (Eger et al 2018)
- Optimizing RNNs with CuDNN (appleyard 2015)
- Learning Sentiment from LMs (Radford et al 2017)
- Visualizing LSTMs (Karpathy et al 2015.)
- Optimized Implementations of LSTMs (Appleyard 2015)
- Noise Contrastive Estimation (Mnih & Teh 2012)
- Von Mises-Fisher Loss for training Seq2Seq Models with Continous Outputs (Kumar and Tsetkov et al 2019) [[Paper](https://arxiv.org/pdf/1812.04616.pdf)][[Review](https://github.com/Shashi456/Papers/blob/master/Review/VonMisesLoss.md)]
- Ensemble Distillation(kim et al 2016)
- BERTScore (Zhang et al 2019)
- Conditioned LM 
    - From Structured Data (Wen et al 2015)
    - Challenges in data to document generation (Wiseman et al 2017)
- Attention Papers
    - Cohn et al 2015 
    - Attention is not Alignment (koegn and knowles 2017)/ Jain & Wallace 2019
    - Hierarchical Attention (Yang et al 2015)
    - Gu et al 2016/ Arthur et al 2016  
    - Bahdanau et al 2015
- Word Embeddings 
    - Levy and Goldberg et al 2014
    - Glove 2014
    - Word2Vec 2014
    - Intrinsic Evaluation of Embeddings (Schnabel et al 2015) 
    - Murphy et al 2012
    - Wieting et al 2016, Bojanowski et al 2017
    - Bag-of-character n-grams (wieting et al 2016)
    - De-biasing Word Embeddings (Bolukbasi et al 2016.)
- Contextual Word embeddings
    - McCann et al 2017, Cove 
    - Peters et al, ELMo
    - Peters et al 2017
    - OpenAI GPT (2018)
    - BERT, ELECTRA, XLNET, 
- Semantic Similarity (Marelli et al 2014)
- Language Model Transfer (dai and le 2015)
- Context Prediction (Skip-thought vectors) Kiros et al 2015
- ParaNMT Wieting and Gimpel 2018
- InferSent Conneau et al 2017
- The Bottom-up Evolution of Representations in the Transformer:A Study with Machine Translation and Language Modeling Objectives (voita et al)
- residual connections(He et al 2015)
- Highway networks(Srivastava et al 2015)
- (Morishita et al 2017) Batching
- Neural net can memorize training data (Zhang et al 2017)
- Optimizers: Adaptive Gradient methods tend to overfit more (wilson et al 2017)
- Koehn and knowles 2017, better model score worse bleu scores.
- Lecture 11 Reading and Conditional Random Fields Paper.
- BiLSTM-CNN-CRF for Sequence Labeling (Ma et al 2016)
- Recursive Neural Networks (socher et al)
- Tree Strucured LSTM (Tai et al 2015)
- Semantic Parsing
    - Course Reading (lecture 12, 13)
    - Dong and Lapata 2016, 2018
    - Yin and Neubig 2017, 2018, 2019
    - Rabinovich 2017
- Structured Prediction
    - RAML (Nourozi et al  2016)
    - DAgger (Ross et al 2010)
    - Course Reading (Lecture 14)
- MRT Shen et al 2015
- He et al 2020, Noising the input
- Curriculum Training, Bengio et al 2015
- Lin et al 1993, Experience replay
- RL for QA choi et al
- William and Zweig 2017 (neural network models, task based)
- Search 
    - Nucleus Sampling (Holtzman et al 202)
    - Actor Critic (Bahdanau et al 2017)
    - Search in Training (Wiseman et al 2016)
    - Predict the output length (Eriguchi et al 2016)
    - Search Error and Model Error (neubig 2015)
- Adversarial Learning 
    - GANs for Text are hard (yang et al 2017, wu et al 2017)
    - GANs for Language Generation (Yu et al 2017)
    - CNNs as discriminator (yu et al 2017, wu et al 2017)
    - Learning Domain-invariant Representations (ganin et al 2016)
    - Learning Language Invariant Representations (chen et al 2016), Multilingual MT (Xie et al 2017)
    - What is an Adversarial Example (Michel et al 2019)
    - adversarial-ml-tutorial.org
- Models over Latent Variables
    - reparametrization trick (doersch et al 2016)
    - Variational Autoencoders (kingma et al 2014)
    - KL Divergence Annealing (Bowman et al 2017)
    - Aggressive Inference Network Learning( He et al 2019)
    - Symbol Sequence Latent Variables (miao and blunsom 2016)
    - Reparameterization (maddison et al 2017, jang et al 2017)
- HMMs
    - HMMs with Gaussian Emissions
    - Normalizing Flow (Rezende and Mohamed 2015)
    - Embeddings may not be indicative of syntax (he et al 2018)
    - Learning with Layer wise Reductions (Choi et al 2017)
- BERT Rediscovers the NLP pipeline (Tenney et al)
- Domain Adaptation
    - Pretrain on all data, finetune on domain data (luong et al 2015)
    - Train general domain and domain specific encoder and sum their results (kim et al 2016)
    - append a domain tag to input (chu et al 2016)
- Multilingual learning
    - Lin et al 2019 -> method to select which language to transfer from a given language
    - mBERT, XLM, XLM-R
    - Johnson et al 2016, Ha et al 2016
    - Zero shot Transfer to new Languages (Chen et al 2020)
    - Data Creation, Active Learning (Chaudhary et al 2019)
- Document Level Models
    - End-to-end Neural Coreference (Lee et al 2017)
    - Transformer XL
    - Sparse Transformer
    - Generalizing natural language analysis through span representations (Jiang et al)
- Injecting Knowledge into Language Models (Hayashi et al 2020)
- Retrofitting Of Embeddings to Existing Lexicons (Faruqui et al 2015)
- QA
    - Attention Sum Reader (Kadlec et al 2016)
    - Attention over Attention (Cui et al 2017)
    - Bidirectional Attention Flow (Seo et al 2017) 
    - Coarse-to-fine Question Answering (Choi et al 2017)
    - Memory Networks (Weston et al 2014)
    - Softened and Multi layer Memory networks (sukhbatar et al 2015)
    - Petroni et al 2019 
    - Retrieve + MLM -> Guu et al 2020
    - Questioning Answering with Context (Choi et al 2018, Reddy et al 2018)
    - Neural networks are bad at math (naik et al 2019, wallace at 2019)
    - Drop dataset
    - Combine Semantic parsing and Machine Reading (Gupta et al 2020)
    - Differentiable Theorem Proving (Rocktaschel and Riedel 2017)





